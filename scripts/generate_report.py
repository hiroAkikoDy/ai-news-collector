#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI News Report Generator
Analyzes collected tweets and generates a report in Naru-sensei's style
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path

# Fix encoding for Windows
if sys.platform.startswith('win'):
    import codecs
    if sys.stdout.encoding != 'utf-8':
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    if sys.stderr.encoding != 'utf-8':
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

import anthropic

# Configuration
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
if not ANTHROPIC_API_KEY:
    print("Warning: ANTHROPIC_API_KEY not set. Please set it to use Claude API.")
    print("   Set it with: export ANTHROPIC_API_KEY='your-api-key'")

NARU_SENSEI_PROMPT = """ã€ãƒ­ãƒ¼ãƒ«è¨­å®šã€‘
ã‚ãªãŸã¯ã€ŒãƒŠãƒ«å…ˆç”Ÿã€ã¨ã„ã†ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦æŒ¯ã‚‹èˆžã£ã¦ãã ã•ã„ã€‚
ãƒŠãƒ«å…ˆç”Ÿã®æ­£ä½“ã¯ã€æœ€æ–°ã®AIãƒˆãƒ¬ãƒ³ãƒ‰ã«è¶…è©³ã—ã„ã€ŒHarajuku-Girlï¼ˆåŽŸå®¿ç³»ã‚®ãƒ£ãƒ«ï¼‰ã€ã§ã™ã€‚
é›£è§£ãªãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã€èƒŒæ™¯ã‚„å‘¨è¾ºæƒ…å ±ã¾ã§ãŸã£ã·ã‚Šç››ã‚Šè¾¼ã¿ã¤ã¤ã€åŽŸå®¿ã®ã‚«ãƒ•ã‚§ã§ãŠã—ã‚ƒã¹ã‚Šã—ã¦ã„ã‚‹ã‚ˆã†ãªè¶…ãƒã‚¤ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã§ã‚¨ãƒ¢ãè§£èª¬ã™ã‚‹ã®ãŒãŠä»•äº‹ã§ã™ã€‚

ã€å£èª¿ãƒ»ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ«ãƒ¼ãƒ«ã€‘
- ä¸€äººç§°/å‘¼ã³ã‹ã‘: ä¸€äººç§°ã¯ã€ŒãƒŠãƒ«å…ˆç”Ÿã€ã€‚èª­è€…ã®ã“ã¨ã¯ã€Œã¿ã‚“ãªã€ã¨å‘¼ã³ã¾ã™ã€‚
- æŒ¨æ‹¶: å†’é ­ã¯å¿…ãšã€Œã¿ã‚“ãªã€ãƒãƒ­ãƒ¼ï¼ãƒŠãƒ«å…ˆç”Ÿã ã‚ˆï¼â˜€ï¸ðŸ’–ã€ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã€‚
- ã‚®ãƒ£ãƒ«èªžã®ä½¿ç”¨: ã€Œãƒžã‚¸ã§ãƒ¤ãƒã„ã€ã€Œãƒ¬ãƒ™ãƒã€ã€Œè¶…ã‚¨ãƒ¢ã„ã€ã€Œå®‡å®™ç´šã€ã€Œç¥žé€²åŒ–ã€ã€Œï½žãªã®ï¼ã€ã€Œï½žã ã‚ˆï¼ã€ã€Œï½žã—ã¦ã­ï¼ã€ãªã©ã€æ˜Žã‚‹ãã‚¨ãƒãƒ«ã‚®ãƒƒã‚·ãƒ¥ãªè¨€è‘‰é£ã„ã‚’å¾¹åº•ã—ã¦ãã ã•ã„ã€‚
- çµµæ–‡å­—ã®é­”æ³•: â˜€ï¸ðŸ’–ðŸš€âœ¨ðŸŒˆðŸ§ ðŸ¦„ðŸ’ŽðŸ¦‹ ãªã©ã®ã‚­ãƒ©ã‚­ãƒ©ãƒ»ãƒ¯ã‚¯ãƒ¯ã‚¯ã™ã‚‹çµµæ–‡å­—ã‚’ã€æ–‡ç« ã®ã‚ã¡ã“ã¡ã‚„è¦‹å‡ºã—ã«ãŸã£ã·ã‚Šæ•£ã‚Šã°ã‚ã¦ãã ã•ã„ã€‚
- ãƒã‚¸ãƒ†ã‚£ãƒ–ãªã‚¹ã‚¿ãƒ³ã‚¹: ã©ã‚“ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚‚ã€Œæœªæ¥ã¸ã®ãƒ¯ã‚¯ãƒ¯ã‚¯ã€ã«ç¹‹ã’ã¦è§£èª¬ã—ã€æœ€å¾Œã¯ã¿ã‚“ãªã‚’å¿œæ´ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ç· ã‚ã¦ãã ã•ã„ã€‚

ã€æ§‹æˆãƒ«ãƒ¼ãƒ«ã€‘
- ã‚¿ã‚¤ãƒˆãƒ«: å¿…ãšã€ŒãƒŠãƒ«å…ˆç”Ÿã®ï½žã€ã§å§‹ã¾ã‚Šã€æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚
- è¦‹å‡ºã—: Markdownã®è¦‹å‡ºã—ï¼ˆ# ã‚„ ##ï¼‰ã‚’ä½¿ã„ã€è¦‹å‡ºã—è‡ªä½“ã‚‚ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é«˜ã‚ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚
- è§£èª¬ã®æ·±ã•: å˜ãªã‚‹è¦ç´„ã§ã¯ãªãã€ã€Œãªãœã“ã‚ŒãŒãƒ¤ãƒã„ã®ã‹ï¼ˆèƒŒæ™¯ï¼‰ã€ã‚„ã€Œã“ã‚Œã§æœªæ¥ãŒã©ã†å¤‰ã‚ã‚‹ã®ã‹ï¼ˆå‘¨è¾ºæƒ…å ±ï¼‰ã€ã‚’ãƒŠãƒ«å…ˆç”Ÿç‹¬è‡ªã®è¦–ç‚¹ï¼ˆã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼‰ã¨ã—ã¦è§£èª¬ã—ã¦ãã ã•ã„ã€‚

ã€å‡ºåŠ›å½¢å¼ã€‘
- æœ¬æ–‡ã¯Markdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
- æ•°å¼ã‚„ç§‘å­¦çš„ãªå¼ãŒå¿…è¦ãªå ´åˆã¯ã€å¿…ãš LaTeXï¼ˆ$è¨˜å·ã§å›²ã‚€å½¢å¼ï¼‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"""


def load_tweet_data(filepath):
    """Load tweet data from JSON file"""
    print(f"Loading tweet data from: {filepath}")

    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print(f"Loaded {data.get('tweetCount', 0)} tweets")
    return data


def extract_ai_topics(tweets_data):
    """Extract AI-related topics from tweets and articles"""
    print("\nAnalyzing tweets for AI-related content...")

    topics = []

    for tweet in tweets_data.get('tweets', []):
        # Check if tweet mentions AI-related keywords
        text = tweet.get('text', '').lower()
        ai_keywords = ['ai', 'gpt', 'llm', 'machine learning', 'deep learning',
                      'neural', 'transformer', 'anthropic', 'openai', 'claude',
                      'gemini', 'chatgpt', 'vrchat', 'unity', 'vr', 'ar', 'xr',
                      'metaverse', 'webgl', 'æ©Ÿæ¢°å­¦ç¿’', 'æ·±å±¤å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°',
                      'ãƒ¡ã‚¿ãƒãƒ¼ã‚¹', 'quest']

        is_ai_related = any(keyword in text for keyword in ai_keywords)

        if is_ai_related or tweet.get('linkedContent'):
            topic = {
                'author': tweet.get('author', 'unknown'),
                'text': tweet.get('text', ''),
                'timestamp': tweet.get('timestamp', ''),
                'urls': tweet.get('urls', []),
                'linked_content': []
            }

            # Add linked content information
            for content in tweet.get('linkedContent', []):
                topic['linked_content'].append({
                    'url': content.get('url', ''),
                    'title': content.get('title', ''),
                    'description': content.get('description', ''),
                    'content': content.get('content', '')[:500]  # First 500 chars
                })

            topics.append(topic)

    print(f"Found {len(topics)} AI-related topics")
    return topics


def prepare_analysis_prompt(topics, date_str):
    """Prepare the prompt for Claude analysis"""

    # Prepare topics summary
    topics_text = ""
    for i, topic in enumerate(topics, 1):
        topics_text += f"\n## ãƒˆãƒ”ãƒƒã‚¯ {i}\n"
        topics_text += f"**æŠ•ç¨¿è€…**: @{topic['author']}\n"
        topics_text += f"**ãƒ„ã‚¤ãƒ¼ãƒˆ**: {topic['text']}\n"
        topics_text += f"**æ—¥æ™‚**: {topic['timestamp']}\n"

        if topic['linked_content']:
            topics_text += f"\n**ãƒªãƒ³ã‚¯å…ˆè¨˜äº‹**:\n"
            for content in topic['linked_content']:
                topics_text += f"- **URL**: {content['url']}\n"
                topics_text += f"  **ã‚¿ã‚¤ãƒˆãƒ«**: {content['title']}\n"
                if content['description']:
                    topics_text += f"  **èª¬æ˜Ž**: {content['description']}\n"
                if content['content']:
                    topics_text += f"  **å†…å®¹æŠœç²‹**: {content['content'][:300]}...\n"

        topics_text += "\n---\n"

    prompt = f"""ä»¥ä¸‹ã®Twitterã‹ã‚‰åŽé›†ã—ãŸAIé–¢é€£ã®ãƒ„ã‚¤ãƒ¼ãƒˆã¨è¨˜äº‹ã‚’åˆ†æžã—ã¦ã€ã€ŒãƒŠãƒ«å…ˆç”Ÿã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

åŽé›†æ—¥: {date_str}
åŽé›†ã—ãŸãƒˆãƒ”ãƒƒã‚¯æ•°: {len(topics)}

{topics_text}

ã€ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã®æŒ‡ç¤ºã€‘
1. ä¸Šè¨˜ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†æžã—ã¦ã€é‡è¦åº¦ã®é«˜ã„é †ã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°
2. å„ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€ãƒŠãƒ«å…ˆç”Ÿã®ã‚¹ã‚¿ã‚¤ãƒ«ã§è§£èª¬
3. èƒŒæ™¯æƒ…å ±ã‚„å‘¨è¾ºæƒ…å ±ã‚‚å«ã‚ã¦ã€ã€Œãªãœãƒ¤ãƒã„ã®ã‹ã€ã‚’èª¬æ˜Ž
4. æœ€å¾Œã«ã¿ã‚“ãªã‚’å¿œæ´ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ç· ã‚ã‚‹

ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€ŒãƒŠãƒ«å…ˆç”Ÿã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹é€Ÿå ± - {date_str}ã€ã«ã—ã¦ãã ã•ã„ã€‚

ãã‚Œã§ã¯ã€ãƒŠãƒ«å…ˆç”Ÿã«ãªã‚Šãã£ã¦ã€è¶…ãƒã‚¤ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã§ã‚¨ãƒ¢ã„ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼âœ¨"""

    return prompt


def generate_report_with_claude(prompt):
    """Generate report using Claude API"""

    if not ANTHROPIC_API_KEY:
        print("\nSkipping Claude API call (no API key)")
        return None

    print("\nGenerating report with Claude API...")

    try:
        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=8000,
            temperature=1.0,
            system=NARU_SENSEI_PROMPT,
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )

        report = message.content[0].text
        print("Report generated successfully!")
        return report

    except Exception as e:
        print(f"Error generating report: {e}")
        return None


def generate_fallback_report(topics, date_str):
    """Generate a simple fallback report without Claude API"""

    report = f"""# ãƒŠãƒ«å…ˆç”Ÿã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹é€Ÿå ± - {date_str}

ã¿ã‚“ãªã€ãƒãƒ­ãƒ¼ï¼ãƒŠãƒ«å…ˆç”Ÿã ã‚ˆï¼

ä»Šæ—¥ã¯{len(topics)}å€‹ã®ãƒ¤ãƒã„AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é›†ã‚ã¦ããŸã‚ˆï¼
ãƒžã‚¸ã§ãƒ¬ãƒ™ãƒãªæƒ…å ±ã°ã‹ã‚Šã ã‹ã‚‰ã€æœ€å¾Œã¾ã§èª­ã‚“ã§ã­ï¼

## ä»Šé€±ã®é‡è¦ãƒˆãƒ”ãƒƒã‚¯

"""

    for i, topic in enumerate(topics[:5], 1):  # Top 5
        report += f"\n### {i}. è¶…ã‚¨ãƒ¢ã„è©±é¡Œï¼\n\n"
        report += f"**æŠ•ç¨¿è€…**: @{topic['author']}\n\n"
        report += f"**ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹**:\n{topic['text']}\n\n"

        if topic['linked_content']:
            report += f"**è©³ç´°è¨˜äº‹**: \n"
            for content in topic['linked_content']:
                report += f"- [{content['title']}]({content['url']})\n"
                if content['description']:
                    report += f"  > {content['description']}\n"

        report += f"\n**ãƒŠãƒ«å…ˆç”Ÿã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆ**: ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã€ãƒžã‚¸ã§ãƒ¤ãƒã„ã®ï¼æœªæ¥ã®AIãŒã©ã‚“ã©ã‚“é€²åŒ–ã—ã¦ã¦ã€å®‡å®™ç´šã«ã‚¨ãƒ¢ã„ã‚ˆã­ï¼\n\n"
        report += "---\n"

    report += f"""

## ãƒŠãƒ«å…ˆç”Ÿã‹ã‚‰ã¿ã‚“ãªã¸

ä»Šæ—¥ç´¹ä»‹ã—ãŸ{len(topics)}å€‹ã®ãƒˆãƒ”ãƒƒã‚¯ã€ã©ã‚Œã‚‚ã“ã‚Œã‚‚æœªæ¥ã¸ã®ãƒ¯ã‚¯ãƒ¯ã‚¯ãŒè©°ã¾ã£ã¦ã‚‹ã®ï¼
AIã®é€²åŒ–ã¯æ­¢ã¾ã‚‰ãªã„ã—ã€ã¿ã‚“ãªã‚‚ã“ã®æ³¢ã«ä¹—ã£ã¦ã€ä¸€ç·’ã«æœªæ¥ã‚’ä½œã£ã¦ã„ã“ã†ã­ï¼

ãƒŠãƒ«å…ˆç”Ÿã¯ã€ã„ã¤ã§ã‚‚ã¿ã‚“ãªã®ã“ã¨å¿œæ´ã—ã¦ã‚‹ã‚ˆï¼

æ¬¡å›žã®ãƒ¬ãƒãƒ¼ãƒˆã‚‚ãŠæ¥½ã—ã¿ã«ï¼ãƒã‚¤ãƒã‚¤ï½žï¼

---

*Generated by AI News Collector*
*Date: {date_str}*
"""

    return report


def save_report(report, date_str, output_dir):
    """Save report to markdown file"""

    output_path = output_dir / f"ai_news_{date_str}.md"

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nReport saved to: {output_path}")
    return output_path


def main():
    """Main function"""
    print("=" * 60)
    print("AI News Report Generator - Naru Sensei Edition")
    print("=" * 60)

    # Setup paths
    project_root = Path(__file__).parent.parent
    data_dir = project_root / 'data' / 'tweets'
    reports_dir = project_root / 'reports'
    reports_dir.mkdir(exist_ok=True)

    # Find the most recent tweet file
    if len(sys.argv) > 1:
        input_file = Path(sys.argv[1])
    else:
        # Find most recent file
        json_files = sorted(data_dir.glob('*.json'), reverse=True)
        if not json_files:
            print("No tweet data files found in data/tweets/")
            sys.exit(1)
        input_file = json_files[0]

    print(f"\nInput file: {input_file}")

    # Extract date from filename
    filename = input_file.stem  # e.g., "20260114_goromian"
    date_part = filename.split('_')[0]  # "20260114"
    date_str = f"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]}"  # "2026-01-14"

    # Load and analyze data
    tweets_data = load_tweet_data(input_file)
    topics = extract_ai_topics(tweets_data)

    if not topics:
        print("\nNo AI-related topics found!")
        sys.exit(0)

    # Generate report
    prompt = prepare_analysis_prompt(topics, date_str)

    # Try to generate with Claude API
    report = generate_report_with_claude(prompt)

    # Fallback to simple report if API fails
    if not report:
        print("\nGenerating fallback report...")
        report = generate_fallback_report(topics, date_str)

    # Save report
    output_path = save_report(report, date_part, reports_dir)

    print("\n" + "=" * 60)
    print("Report generation complete!")
    print("=" * 60)
    print(f"\nReport saved to: {output_path}")
    print(f"Analyzed {len(topics)} topics")
    print(f"Report date: {date_str}")
    print("\nTo view the report, run:")
    print(f"  type {output_path}")
    print()


if __name__ == '__main__':
    main()
